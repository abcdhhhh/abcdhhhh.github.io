<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://abcdhhhh.github.io/atom.xml" rel="self"/>
  
  <link href="http://abcdhhhh.github.io/"/>
  <updated>2021-01-30T02:32:09.412Z</updated>
  <id>http://abcdhhhh.github.io/</id>
  
  <author>
    <name>abcdhhhh</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>图的计数</title>
    <link href="http://abcdhhhh.github.io/2021/01/29/%E5%9B%BE%E7%9A%84%E8%AE%A1%E6%95%B0/"/>
    <id>http://abcdhhhh.github.io/2021/01/29/%E5%9B%BE%E7%9A%84%E8%AE%A1%E6%95%B0/</id>
    <published>2021-01-29T14:45:43.000Z</published>
    <updated>2021-01-30T02:32:09.412Z</updated>
    
    <content type="html"><![CDATA[<h1 id="无标号计数"><a href="#无标号计数" class="headerlink" title="无标号计数"></a>无标号计数</h1><h2 id="1、二叉树"><a href="#1、二叉树" class="headerlink" title="1、二叉树"></a>1、二叉树</h2><p>设OGF为$H(x)=\sum_{i=0}^{\infty}h_ix^i$。<br>任意一棵大小为$n(n\ge 1)$二叉树去掉根，可以得到两棵大小之和为$n-1$的二叉树。因此$h_n=\sum_{i=0}^{n-1}h_ih_{n-1-i}(n\ge 1)$。</p><p>根据递推关系可列出方程$xH(x)^2-H(x)+1=0$。<br>解得$H(x)=\frac{1-\sqrt{1-4x}}{2x}$。<br>求得$h_n=\frac{\binom{2n}{n}}{n+1}$。</p><h2 id="预备知识：Euler变换"><a href="#预备知识：Euler变换" class="headerlink" title="*预备知识：Euler变换"></a>*预备知识：Euler变换</h2><p>Euler变换在无标号计数中的组合意义相当于有标号计数中的exp，即阶数之和为$n$的结构的任意组合。</p><p>设$F(x)=\sum_{i=0}^{\infty}f_ix^i$。<br>定义$\mathcal{E}(F(x))=\prod_{i=0}^\infty \frac{1}{(1-x^i)^{f_i}}$。<br>上式可化为$\mathcal{E}(F(x))=\exp(\sum_{i=1}^\infty\frac{F(x^i)}{i})$。</p><h2 id="2、有根树"><a href="#2、有根树" class="headerlink" title="2、有根树"></a>2、有根树</h2><p>设OGF为$F(x)=\sum_{i=0}^\infty f_ix^i$。<br>任意一棵大小为$n(n\ge 1)$的有根树去掉根，可以得到若干棵大小之和为$n-1$的有根树。因此$F(x)=x\mathcal{E}(F(x))$。</p><h2 id="2-、无根树"><a href="#2-、无根树" class="headerlink" title="2*、无根树"></a>2*、无根树</h2><p>设有根树的OGF为$F(x)=\sum_{i=0}^\infty f_ix^i$，无根树的OGF为$H(x)=\sum_{i=0}^\infty h_ix^i$。<br>要统计无根树，只需统计以重心为根的有根树。根据重心的定义一个点是重心当且仅当它的每棵子树大小不超过$[\frac{n}{2}]$。<br>因此，一棵树有2个重心当且仅当重心最大子树的大小<strong>恰好</strong>是$\frac{n}{2}$，否则只有1个重心。当树有两个重心时，也要把重心重复计算的情况给减去。<br>因此，$h_n=\begin{cases}f_n-\sum_{k=\frac{n+1}{2}}^{n-1}f_kf_{n-k}&amp;n为奇数\\f_n-\sum_{k=\frac{n}{2}+1}^{n-1}f_kf_{n-k}-\binom{f_{\frac{n}{2}}}{2}&amp;n为偶数\end{cases}$。<br>即$H(x)=F(x)-\frac{1}{2}F(x)^2+F(x^2)$。</p><h1 id="有标号计数"><a href="#有标号计数" class="headerlink" title="有标号计数"></a>有标号计数</h1><h2 id="1、无根树"><a href="#1、无根树" class="headerlink" title="1、无根树"></a>1、无根树</h2><p>由<a href=https://oi-wiki.org/graph/prufer/>Prufer序列</a>和无根树的一一对应关系，得答案$n^{n-2}$。</p><h2 id="1-、有根树"><a href="#1-、有根树" class="headerlink" title="1*、有根树"></a>1*、有根树</h2><p>答案$n^{n-1}$。</p><h2 id="2、无向连通图"><a href="#2、无向连通图" class="headerlink" title="2、无向连通图"></a>2、无向连通图</h2><p>设$n$阶有标号无向连通图有$f_n$种。<br>设$F(x)=\sum_{i=0}^\infty \frac{f_i}{i!}x^i$。<br>则$\exp F(x)=\sum_{i=0}^\infty\frac{2^\binom{i}{2}}{i!}x^i$。<br>即$F(x)=\ln\sum_{i=0}^\infty\frac{2^\binom{i}{2}}{i!}x^i$</p><h2 id="3、点双连通图"><a href="#3、点双连通图" class="headerlink" title="3、点双连通图"></a>3、点双连通图</h2><p>设无向连通图的EGF为$F(x)$，有根无向连通图的EGF为$D(x)$，则$[x^n]F(x)=n[x^n]D(x)$。<br>设点双连通图的EGF为$B(x)$。<br>任意一个大小为$n$的有根无向连通图去掉根都可以得到若干个大小之和为$n-1$的连通块。对每个连通块，考虑根所在的点双连通分量，每个点作为根，都可以向外连出一个有根无向连通图。<br>因此一个连通块的EGF为$\sum_{i=1}^\infty \frac{b_{i+1}D^i(x)}{i!}=B’(D(x))$。<br>因此$D(x)=\exp B’(D(x))$。</p><h2 id="4、边双连通图"><a href="#4、边双连通图" class="headerlink" title="4、边双连通图"></a>4、边双连通图</h2><p>设有根无向连通图的EGF为$D(x)$，边双连通图的EGF为$B(x)$。<br>考虑根所在的边双连通分量，每个点都可以向外连接若干个有根无向连通图的根。<br>因此$D(x)=\sum_{i=1}^\infty \frac{b_ix^i\exp iD(x)}{i!}=B(x\exp D(x))$</p><h2 id="5、仙人掌"><a href="#5、仙人掌" class="headerlink" title="5、仙人掌"></a>5、仙人掌</h2><p>设EGF为$F(x)=\sum_{i=0}^\infty \frac{f_i}{i!}x^i$。<br>$F=x\exp (F+\frac{1}{2}\sum_{i=2}^\infty F^i)=x\exp \frac{F(2-F)}{2(1-F)}$</p><h2 id="6、DAG"><a href="#6、DAG" class="headerlink" title="6、DAG"></a>6、DAG</h2><p>设EGF为$F(x)=\sum_{i=0}^\infty \frac{f_i}{i!}x^i$。<br>$f_n=\sum_{i=1}^n(-1)^{i-1}\binom{n}{i}2^{i(n-i)}f_{n-i}$。</p><h2 id="7、欧拉图"><a href="#7、欧拉图" class="headerlink" title="7、欧拉图"></a>7、欧拉图</h2><p>设EGF为$F(x)=\sum_{i=0}^\infty \frac{f_i}{i!}x^i$。每个点度数均为偶数的图的EGF为$G(x)=\sum_{i=0}^\infty \frac{g_i}{i!}x^i$。<br>对于任意一个$n-1$阶的图，都可以新加一个点$n$，与图中所有度数为奇数的点连边。<br>因此$g_n=2^\binom{n-1}{2}$。<br>而$G(x)=\exp F(x)$。<br>因此$F(x)=\ln G(x)$。</p>]]></content>
    
    
    <summary type="html">永远学不会的多项式，永远理不清的递推关系</summary>
    
    
    
    <category term="算法" scheme="http://abcdhhhh.github.io/categories/%E7%AE%97%E6%B3%95/"/>
    
    
  </entry>
  
  <entry>
    <title>监督学习</title>
    <link href="http://abcdhhhh.github.io/2021/01/29/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/"/>
    <id>http://abcdhhhh.github.io/2021/01/29/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0/</id>
    <published>2021-01-29T14:45:43.000Z</published>
    <updated>2021-01-31T01:56:34.140Z</updated>
    
    <content type="html"><![CDATA[<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p><pre class="mermaid">graph TD;ts(Training set);la(Learning algorithm);h(hypothesis function);1((x))2((y))ts-->la-->h;1-->h-->2;</pre></p><h1 id="训练集"><a href="#训练集" class="headerlink" title="训练集"></a>训练集</h1><p>$\{(x^{(i)},y^{(i)})|i=1,2,…,m\}$<br>$x_j^{(i)}$表示第$i$个训练数据的第$j$个特征。<br>记$X=\begin{bmatrix}x^{(1)}&amp;x^{(2)}&amp;…&amp;x^{(m)}\end{bmatrix}^T=\begin{bmatrix}x_0^{(1)}&amp;x_1^{(1)}&amp;…&amp;x_n^{(1)}\\x_0^{(2)}&amp;x_1^{(2)}&amp;…&amp;x_n^{(2)}\...&amp;…&amp;…&amp;…\\x_0^{(m)}&amp;x_1^{(m)}&amp;…&amp;x_n^{(m)}\end{bmatrix}$，$y=\begin{bmatrix}y^{(1)}\\y^{(2)}\...\\y^{(m)}\end{bmatrix}$。</p><h2 id="标准化"><a href="#标准化" class="headerlink" title="标准化"></a>标准化</h2><p>$x_i:=\frac{x_i-\mu_i}{s_i}$<br>其中$\mu_i$为平均值，$s_i$为标准差。</p><h1 id="回归问题——线性回归"><a href="#回归问题——线性回归" class="headerlink" title="回归问题——线性回归"></a>回归问题——线性回归</h1><p>令$x_0=1$，$x=\begin{bmatrix}x_0\\x_1\...\\x_n\end{bmatrix}$，$\theta=\begin{bmatrix}\theta_0\\\theta_1\...\\\theta_n\end{bmatrix}$。<br>假设函数：$h_\theta (x)=\theta^Tx$。</p><p>记$h=\begin{bmatrix}h_\theta({x^{(1)})}\\h_\theta({x^{(2)})}\...\\h_\theta({x^{(m)})}\end{bmatrix}=X\theta$。<br>总代价函数：$J(\theta)=\frac{1}{2m}||h-y||^2=\frac{1}{2m}||X\theta-y||^2$</p><h2 id="tip-多项式回归转线性回归"><a href="#tip-多项式回归转线性回归" class="headerlink" title="tip: 多项式回归转线性回归"></a>tip: 多项式回归转线性回归</h2><p>将$x_1^2, x_1x_2^3$等高次项也作为特征。</p><h2 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p>不断迭代：$\theta :=\theta-\alpha \nabla J(\theta)$<br>其中：</p><ul><li>$\alpha$为学习速率</li><li>$\nabla J(\theta)=\frac{1}{m}X^T(X\theta-y)$为梯度</li></ul><h2 id="一般方程"><a href="#一般方程" class="headerlink" title="一般方程"></a>一般方程</h2><p>$\theta=(X^TX)^{-1}X^Ty$<br>当$X^TX$不可逆时，需思考特征是否过多（比如$m\le n$）。</p><h1 id="二分类问题——logistic回归"><a href="#二分类问题——logistic回归" class="headerlink" title="二分类问题——logistic回归"></a>二分类问题——logistic回归</h1><p>sigmoid函数：$g(z)=\frac{1}{1+e^{-z}}$。<br>hypothesis函数：$h_\theta (x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}=P(y=1|x;\theta)$。<br>记$h=g(X\theta)$。</p><p>决策函数：$y=[h_\theta(x)&gt;0.5]$<br>决策边界： $h_\theta(x)=0.5$，即$\theta^Tx=0$。</p><p>代价函数：$\text{Cost}(h_\theta(x),y)=\begin{cases}-\log(h_\theta(x))&amp;y=1\-\log(1-h_\theta(x))&amp; y=0\end{cases}\\=-y\log(h_\theta(x))-(1-y)\log(1-h_\theta(x))$</p><p>总代价函数：$J(\theta)=\frac{1}{m}\sum_{i=1}^m\text{Cost}(h_\theta(x^{(i)}),y^{(i)})\\=-\frac{1}{m}(y^T\log(h)+(1-y)^T\log(1-h))\\=\frac{1}{m}(y^T\log(1+e^{-X\theta})+(1-y)^T\log(1+e^{X\theta}))$</p><h2 id="梯度下降法-1"><a href="#梯度下降法-1" class="headerlink" title="梯度下降法"></a>梯度下降法</h2><p>不断迭代：$\theta :=\theta-\alpha \nabla J(\theta)$<br>其中：</p><ul><li>$\nabla J(\theta)=\frac{1}{m}X^T(h-y)$<h2 id="tip-多分类转二分类"><a href="#tip-多分类转二分类" class="headerlink" title="tip: 多分类转二分类"></a>tip: 多分类转二分类</h2>面对每个新的数据集，都逐类进行检验（把要检验的类当作1，其余当作0），选择可能性最高的那类。<h1 id="欠拟合与过拟合"><a href="#欠拟合与过拟合" class="headerlink" title="欠拟合与过拟合"></a>欠拟合与过拟合</h1>欠拟合：与训练集数据偏差较大。<br>过拟合：较好（甚至完美）地贴合训练集数据，但对训练集以外数据表现不佳。<br>解决过拟合的方法：<ol><li>减少特征数目</li><li>正则化</li></ol></li></ul><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>增加penalty以控制$\theta$规模。（$\theta_0$除外）<br>回归问题<br>$J(\theta)=\frac{1}{2m}||h-y||^2+\frac{\lambda}{2m}(||\theta||^2-\theta_0^2)$<br>分类问题</p><p>$J(\theta)=-\frac{1}{m}(y^T\log(h)+(1-y)^T\log(1-h))+\frac{\lambda}{2m}(||\theta||^2-\theta_0^2)$</p><p>$\theta_j :=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)} (j\ne 0)$<br>（若$\lambda$过大，会导致$\theta$过小，从而导致欠拟合。）<br>一般方程<br>$\theta=(X^TX+\lambda \begin{bmatrix}0&amp;O\\O&amp;I_n\end{bmatrix})^{-1}X^Ty$<br>（还可以解决矩阵不可逆问题）</p>]]></content>
    
    
    <summary type="html">举一反三的原理</summary>
    
    
    
    <category term="机器学习" scheme="http://abcdhhhh.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>神经网络</title>
    <link href="http://abcdhhhh.github.io/2021/01/29/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://abcdhhhh.github.io/2021/01/29/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</id>
    <published>2021-01-29T14:45:43.000Z</published>
    <updated>2021-02-01T03:09:01.256Z</updated>
    
    <content type="html"><![CDATA[<h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><p><img src="神经网络.assets/image-20210131084941074.png" alt="image-20210131084941074"></p><p>神经网络共有$L$层，第$j$层$a^{(j)}$有$s_j$个unit（相当于feature），其中第$i$个unit记为$a_i^{(j)}$。</p><p>二分类问题$s_L=1$，多（$K\ge 3$）分类问题$S_L=K$。</p><p>特别地，记$x=a^{(1)}, h_\Theta(x)=a^{(L)}$。</p><p>第$j$层到第$j+1$层的转移是Logistic回归：$a^{(j+1)}=g(\Theta^{(j)}\begin{bmatrix}a_0^{(j)}\\a^{(j)}\end{bmatrix})$。</p><p>其中</p><ul><li>$a_0^{(j)}=1$为bias unit。</li><li>$g(z)$为sigmoid function: $g(z)=\frac{1}{1+e^{-z}}$。（目的是使得数值接近0或1）</li><li>$\Theta^{(j)}$是一个$s_{j+1}\times (s_j+1)$的矩阵。</li></ul><h2 id="训练集"><a href="#训练集" class="headerlink" title="训练集"></a>训练集</h2><p>$\{(x^{(i)},y^{(i)})|i=1,2,…,m\}$<br>$x_j^{(i)}$表示第$i$个训练数据的第$j$个特征，特征数$n=s_1$。</p><p>记$X=\begin{bmatrix}x^{(1)}&amp;…&amp;x^{(m)}\end{bmatrix}^T=\begin{bmatrix}x_1^{(1)}&amp;…&amp;x_n^{(1)}\\x_1^{(2)}&amp;…&amp;x_n^{(2)}\...&amp;…&amp;…\\x_1^{(m)}&amp;…&amp;x_n^{(m)}\end{bmatrix}$，$Y=\begin{bmatrix}y^{(1)}&amp;…&amp;y^{(m)}\end{bmatrix}^T=\begin{bmatrix}y_1^{(1)}&amp;…&amp;y_K^{(1)}\\y_1^{(2)}&amp;…&amp;y_K^{(2)}\...&amp;…&amp;…\\y_1^{(m)}&amp;…&amp;y_K^{(m)}\end{bmatrix}$，$H=h_\Theta(X^T)^T$。</p><p>代价函数：</p><p>$J(\Theta)=-\frac{1}{m}tr(Y^T\log H+(1-Y)^T\log(1-H))+\frac{\lambda}{2m}\sum_{l=1}^{L-1}(||\Theta^{(l)}||^2-||\Theta e_0||^2)$</p><p><img src="神经网络.assets/image-20210131094718226.png" alt="image-20210131094718226"></p><p>求$\min_\Theta J(\Theta)$需要的信息：</p><ul><li>$J(\Theta)$</li><li>$\frac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta)$</li></ul><h1 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h1><p>记$\delta_j^{(l)}$为计算$a_j^{(l)}$出现的偏差。</p><p>$\delta^{(L)}=h_\Theta(x)-y$</p><p>$\delta^{(l)}=(\Theta^{(l)})^T\delta^{(l+1)}.*g’(z^{(l)})$</p><p>$g’(z^{(l)})=a^{(l)}.*(1-a^{(l)})$</p><p>算法流程：</p><blockquote><p>Training set: $\{(x^{(i)},y^{(i)})|i=1,2,…,m\}$</p><p>set $\Delta^{(l)}=O_{s_{l+1}\times s_l}$</p><p>for i = 1:m</p><p>​    set $a^{(1)}=x^{(i)}$</p><p>​    compute $a^{(l)}(l=2,3,…,L)$</p><p>​    compute $\delta^{(l)}(l=L,L-1,…,2)$</p><p>​    $\Delta^{(l)}:=\Delta^{(l)}+\delta^{(l+1)}a^{(l)T}$</p><p>end</p></blockquote><p>则$\frac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta)=D_{i,j}^{(l)}=\Delta_{i,j}^{(l)}+\lambda\Theta_{i,j}^{(l)}[j\ne 0]$。</p>]]></content>
    
    
    <summary type="html">多层结构</summary>
    
    
    
    <category term="机器学习" scheme="http://abcdhhhh.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
  <entry>
    <title>小记</title>
    <link href="http://abcdhhhh.github.io/2021/01/29/%E5%B0%8F%E8%AE%B0/"/>
    <id>http://abcdhhhh.github.io/2021/01/29/%E5%B0%8F%E8%AE%B0/</id>
    <published>2021-01-29T10:08:45.000Z</published>
    <updated>2021-01-30T02:46:01.601Z</updated>
    
    <content type="html"><![CDATA[<p>没啥特别的目的，主要想体验一下建站是一种什么感觉<del>以及不喜欢CSDN的界面风格</del>。<br>关于写啥内容，暂时还没啥明确的想法。希望能早日写出一些有价值的东西吧。</p><p>没有太多可说的，那就放一首歌吧。</p><iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=1389794615&auto=1&height=66"></iframe>]]></content>
    
    
    <summary type="html">写在建站之后</summary>
    
    
    
    <category term="随笔" scheme="http://abcdhhhh.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
</feed>
